{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CARND - Traffic Sign Classification Project\n",
    "==================================\n",
    "German traffic sign classification  as a part of the Self-driving Car Nanodegree program at Udacity using LeNet architecture."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----------\n",
    "### Step 0: Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "import pandas as pd\n",
    "import random\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import math\n",
    "\n",
    "from src import loading, utils, lenet, preprocessing, augmentation\n",
    "from sklearn.utils import shuffle\n",
    "logger = utils.get_logger('Main Notebook')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training, validation, test = loading.load_all()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------\n",
    "### Step 1: Dataset Summary & Exploration\n",
    "\n",
    "Task: provide a basic summary of the dataset, including an exploratory visualisation of the data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary = utils.get_summary([training, validation, test])\n",
    "print(summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.subplot(211)\n",
    "plt.hist(training.y, bins=summary['total-no-of-classes']);\n",
    "plt.title('Training labels distribution');\n",
    "\n",
    "plt.subplot(212)\n",
    "plt.hist(validation.y, bins=summary['total-no-of-classes']);\n",
    "plt.title('Validation labels distribution');\n",
    "plt.tight_layout()\n",
    "plt.savefig('./docs/images/training-validation-histogram.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show that training/validation is somehow balanced\n",
    "y_train_counts = utils.group_labels_by_counts(training)\n",
    "y_validation_counts = utils.group_labels_by_counts(validation)\n",
    "plt.plot(y_train_counts['counts']/ y_validation_counts['counts']);\n",
    "plt.title('Ratio of training to validation label distribution');\n",
    "plt.xlabel('Label');\n",
    "plt.ylabel('(training example count)/(validation example count)');\n",
    "plt.savefig('./docs/images/training-validation-sample-size-ratios.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images = []\n",
    "labels = []\n",
    "for i in range(50):\n",
    "    index = random.randint(0, training.count)\n",
    "    images.append(training.X[index])\n",
    "    labels.append(utils.to_sign_label(training.y[index]))\n",
    "utils.plot_and_save(images, labels, './docs/images/25-random-images.jpg', 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_images = {\n",
    "    'with bad lighting': [17894, 14044],\n",
    "    'good': [22842, 4220]\n",
    "}\n",
    "images = []\n",
    "labels = []\n",
    "for label, indices in example_images.items():\n",
    "    for i in indices:\n",
    "        images.append(training.X[i])\n",
    "        labels.append(utils.to_sign_label(training.y[i]))\n",
    "\n",
    "print(labels)\n",
    "utils.plot_and_save(images, labels, 'docs/images/cherry-pick-good-bad-images.jpg', 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "\n",
    "## Step 2: Design and Test a Model Architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pre-process and augment dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of enabled data augmenters for training data set\n",
    "TRAINING_DATA_AUGMENTERS = [\n",
    "    augmentation.HorizontalFlipper(),\n",
    "    augmentation.AffineTransformAugmenter(),\n",
    "    augmentation.GaussianBlurAugmenter(),\n",
    "    augmentation.RandomScalerAugmenter()\n",
    "]\n",
    "print('Original: ', utils.get_summary([training]))\n",
    "\n",
    "d_train = augmentation.augment_data_set(training, TRAINING_DATA_AUGMENTERS)\n",
    "print('Augmented: ', utils.get_summary([d_train]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of enabled data pre-processors\n",
    "PRE_PROCESSORS = [\n",
    "    preprocessing.GrayScaleConverter(),\n",
    "    preprocessing.ZNormaliser(),\n",
    "]\n",
    "\n",
    "# Perform pre-processing on augmented training and validation data sets\n",
    "d_train = preprocessing.PreProcessor.apply(d_train, PRE_PROCESSORS)\n",
    "d_validation = preprocessing.PreProcessor.apply(validation, PRE_PROCESSORS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply pre-processing to two of the \"bad\" images to show the results\n",
    "BAD_IMAGE_INDEX = [17894, 14044]\n",
    "\n",
    "bad_x = np.zeros((2, 32, 32, 3))\n",
    "bad_y = np.zeros(2,)\n",
    "for i in range(len(BAD_IMAGE_INDEX)):\n",
    "    bad_x[i,:] = training.X[BAD_IMAGE_INDEX[i]]\n",
    "    bad_y[i] = training.y[BAD_IMAGE_INDEX[i]]\n",
    "bad_examples = loading.DataSet('bad_samples', bad_x, bad_y, len(bad_x))\n",
    "\n",
    "pre_processed = preprocessing.PreProcessor.apply(bad_examples, PRE_PROCESSORS)\n",
    "\n",
    "images = []\n",
    "labels = []\n",
    "for i in range(len(BAD_IMAGE_INDEX)):\n",
    "    images.append(training.X[BAD_IMAGE_INDEX[i]]);\n",
    "    labels.append('Original')\n",
    "\n",
    "    images.append(pre_processed.X[i].squeeze());\n",
    "    labels.append('Pre-processed')\n",
    "utils.plot_and_save(images, labels, './docs/images/bad-images-pre-processed.jpg', 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Initialize Placeholders and setup computation graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "x = tf.placeholder(tf.float32, (None, 32, 32, 1))\n",
    "y = tf.placeholder(tf.int32, (None))\n",
    "mode = tf.placeholder(tf.string, (None))\n",
    "\n",
    "training_operation, accuracy_operation, logits = lenet.setup_graph(x, y, mode)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Start training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.lenet import HYPER_PARAMETERS, Mode\n",
    "logger.info('Hyper-parameters: %s', HYPER_PARAMETERS)\n",
    "\n",
    "# TODO: this override is just for local testing - remove it in the final version\n",
    "HYPER_PARAMETERS['EPOCHS'] = 1\n",
    "\n",
    "saver = tf.train.Saver()\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    num_examples = d_train.count\n",
    "\n",
    "    logger.info(\"Training...\")\n",
    "    for i in range(HYPER_PARAMETERS['EPOCHS']):\n",
    "        X_train, y_train = shuffle(d_train.X, d_train.y)\n",
    "        training_accuracy = 0\n",
    "        for offset in range(0, num_examples, HYPER_PARAMETERS['BATCH_SIZE']):\n",
    "            end = offset + HYPER_PARAMETERS['BATCH_SIZE']\n",
    "            batch_x, batch_y = X_train[offset:end], y_train[offset:end]\n",
    "            sess.run(training_operation, feed_dict={x: batch_x, y: batch_y, mode: Mode.TRAINING.value})\n",
    "\n",
    "        training_accuracy = lenet.evaluation(d_train.X, d_train.y, x, y, mode, accuracy_operation)\n",
    "        validation_accuracy = lenet.evaluation(d_validation.X, d_validation.y, x, y, mode, accuracy_operation)\n",
    "        logger.info(\"EPOCH {} ...\".format(i + 1))\n",
    "        logger.info(\"Accuracy on training dataset = {:.3f}\".format(training_accuracy))\n",
    "        logger.info(\"Validation Accuracy = {:.3f}\".format(validation_accuracy))\n",
    "        print()\n",
    "\n",
    "    saver.save(sess, './data/model/lenet')\n",
    "    logger.info(\"Model saved\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 3: Test a Model on New Images\n",
    "\n",
    "To give yourself more insight into how your model is working, download at least five pictures of German traffic signs from the web and use your model to predict the traffic sign type."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load and Output the Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGE_PATHS = [\n",
    "    \"max-50.jpeg\",\n",
    "    \"max-70.jpeg\",\n",
    "    \"no-stopping.jpeg\",\n",
    "    \"right-only.jpeg\",\n",
    "    \"right-turn-only.jpg\",\n",
    "    \"pedestrians.jpg\"\n",
    "]\n",
    "LABELS = [0, 0, 0, 0, 0, 0]\n",
    "downloaded_images = [utils.read_image_for_lenet('./data/traffic-signs-from-internet/' + p) for p in IMAGE_PATHS]\n",
    "\n",
    "labels = ['{}: {}'.format(l, utils.to_sign_label(l)) for l in LABELS]\n",
    "utils.plot_and_save(downloaded_images, labels, './docs/images/images-from-internet-resized.jpg', 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predict the Sign Type for Each Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_x = np.stack(downloaded_images)\n",
    "d_y = np.array(labels)\n",
    "\n",
    "from_internet = loading.DataSet('From Internet', d_x, d_y, len(d_x))\n",
    "print(utils.get_summary([from_internet]))\n",
    "from_internet = preprocessing.PreProcessor.apply(from_internet, PRE_PROCESSORS)\n",
    "\n",
    "\n",
    "softmax_logits = tf.nn.softmax(logits)\n",
    "top_k = tf.nn.top_k(softmax_logits, k=5)\n",
    "\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    saver2 = tf.train.import_meta_graph('./data/model/lenet.meta')\n",
    "    saver2.restore(sess, \"./data/model/lenet\")\n",
    "    \n",
    "    my_softmax_logits = sess.run(top_k, feed_dict={x: from_internet.X, mode: Mode.PREDICTING.value})\n",
    "    print(my_softmax_logits)\n",
    "\n",
    "#     my_accuracy = evaluate(my_images_normalized, my_labels)\n",
    "#     print(\"Test Set Accuracy = {:.3f}\".format(my_accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analyze performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Calculate the accuracy for these 5 new images. \n",
    "### For example, if the model predicted 1 out of 5 signs correctly, it's 20% accurate on these new images."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Output Top 5 Softmax Probabilities For Each Image Found on the Web"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Print out the top five softmax probabilities for the predictions on the German traffic sign images found on the web. \n",
    "### Feel free to use as many code cells as needed."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
